{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a RGB-Depth fusion architecture for semantic segmentation based on Fully Convolutional Network (FCN) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import cv2\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dropout, Concatenate, Conv2DTranspose, Reshape, Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and Ground-truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset consists of 1100 (per modality) images of road scenes. It is divided into train (600 images), test (200 images) and validation (300 images) datasets.\n",
    "- Change the size of all images into 256*256.\n",
    "- Converting the labels into one hot encoding\n",
    "- Create a DataLoader for loading the files when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(Sequence):\n",
    "    def __init__(self, data_dir, batch_size=32, mode='train'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.onehot_encoder = OneHotEncoder()\n",
    "        \n",
    "        self.rgb_files = sorted(os.listdir(os.path.join(data_dir, mode, 'rgb')))\n",
    "        self.depth_files = sorted(os.listdir(os.path.join(data_dir, mode, 'depth')))\n",
    "        self.label_files = sorted(os.listdir(os.path.join(data_dir, mode, 'label')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.rgb_files) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_rgb_files = self.rgb_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_depth_files = self.depth_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_label_files = self.label_files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        \n",
    "        X_rgb, X_depth, y = self.__generate_data(batch_rgb_files, batch_depth_files, batch_label_files)\n",
    "        \n",
    "        return [X_rgb, X_depth], y\n",
    "    \n",
    "    def __generate_data(self, batch_rgb_files, batch_depth_files, batch_label_files):\n",
    "        X_rgb = np.array([cv2.resize(np.load(os.path.join(self.data_dir, self.mode, 'rgb', filename)), (256, 256)) for filename in batch_rgb_files])\n",
    "        X_rgb = X_rgb / 255.0\n",
    "        X_depth = np.array([cv2.resize(np.load(os.path.join(self.data_dir, self.mode, 'depth', filename)), (256, 256)) for filename in batch_depth_files])\n",
    "        X_depth = self.normalize_depth(X_depth)\n",
    "        X_depth_rgb = np.stack((X_depth,) * 3, axis=-1)\n",
    "\n",
    "        y = np.array([cv2.resize(np.load(os.path.join(self.data_dir, self.mode, 'label', filename)), (256, 256)) for filename in batch_label_files])\n",
    "        y_one_hot = keras.utils.to_categorical(y, num_classes=num_classes)\n",
    "        \n",
    "        return X_rgb, X_depth_rgb, y_one_hot\n",
    "    \n",
    "    def normalize_depth(self, depth):\n",
    "        depth_min = np.min(depth)\n",
    "        depth_max = np.max(depth)\n",
    "        normalized_depth = (depth - depth_min) / (depth_max - depth_min)\n",
    "        return normalized_depth\n",
    "    \n",
    "    def visualize_examples(self, num_examples=5):\n",
    "        fig, axes = plt.subplots(num_examples, 3, figsize=(9, 9))\n",
    "        for i in range(num_examples):\n",
    "            idx = np.random.randint(len(self.rgb_files))\n",
    "            rgb = np.load(os.path.join(self.data_dir, self.mode, 'rgb', self.rgb_files[idx]))\n",
    "            depth = np.load(os.path.join(self.data_dir, self.mode, 'depth', self.depth_files[idx]))\n",
    "            label = np.load(os.path.join(self.data_dir, self.mode, 'label', self.label_files[idx]))\n",
    "\n",
    "            axes[i, 0].imshow(rgb)\n",
    "            axes[i, 0].set_title('RGB')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(depth)\n",
    "            axes[i, 1].set_title('Depth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(label)\n",
    "            axes[i, 2].set_title('Label')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the data prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CustomDataLoader(data_dir='../Datasets/road_scenes', batch_size=32, mode='train')\n",
    "loader.visualize_examples(num_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here, we define the Fully Convolutional Network (FCN) for image segmentaion by fusing RGB and depth images. The network consists of two sterams which each stream having the following layers:\n",
    "\n",
    "    1. Pretrained ResNet50 on imageNet as backbone.\n",
    "    2. We Add two Conv layers with 128 and 256 nodes, respectively. Kernel size (3,3), stride (1,1)\n",
    "    3. Top of the Conv layers, we add dropout layer with 0.2.\n",
    "    4. We then concatenate two streams.\n",
    "    5. Then, we add a transposed convolution layerÂ (Conv2DTranspose)  with Kernel size (64,64), stride (32,32)\n",
    "    6. Finally, add a softmax activation layer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rgb_model(input_shape, for_fusion=True):\n",
    "    rgb_input = Input(shape=input_shape)\n",
    "\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    rgb_features = base_model(rgb_input)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(rgb_features)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    if for_fusion:\n",
    "        return x\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_depth_model(input_shape, for_fusion=True):\n",
    "    depth_input = Input(shape=input_shape)\n",
    "\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    rgb_features = base_model(depth_input)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu')(rgb_features)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fcn(input_shape, trainable_layers):\n",
    "    rgb_input = Input(shape=input_shape)\n",
    "    depth_input = Input(shape=input_shape)\n",
    "    \n",
    "    base_model_rgb = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model_rgb._name = 'pretrained_rgb_model'\n",
    "    \n",
    "    for layer in base_model_rgb.layers:\n",
    "        layer._name = f'{layer.name}_rgb'\n",
    "\n",
    "    base_model_depth = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model_depth._name = 'pretrained_depth_model'\n",
    "\n",
    "    for layer in base_model_depth.layers:\n",
    "        layer._name = f'{layer.name}_depth'\n",
    "\n",
    "    \n",
    "    conv1_rgb = Conv2D(128, (3, 3), activation='relu', padding='same')(base_model_rgb.output)\n",
    "    conv2_rgb = Conv2D(256, (3, 3), activation='relu', padding='same')(conv1_rgb)\n",
    "    conv2_rgb = Dropout(0.2)(conv2_rgb)\n",
    "    \n",
    "    conv1_depth = Conv2D(128, (3, 3), activation='relu', padding='same')(base_model_depth.output)\n",
    "    conv2_depth = Conv2D(256, (3, 3), activation='relu', padding='same')(conv1_depth)\n",
    "    conv2_depth = Dropout(0.2)(conv2_depth)\n",
    "    \n",
    "    concat_features = Concatenate(axis=-1)([conv2_rgb, conv2_depth])\n",
    "    \n",
    "    transposed_conv = Conv2DTranspose(num_classes, kernel_size=(64, 64), strides=(32, 32), padding='same')(concat_features)\n",
    "    \n",
    "    reshaped_output = Reshape((input_shape[0], input_shape[1], num_classes))(transposed_conv)\n",
    "\n",
    "    output = Softmax()(reshaped_output)\n",
    "    \n",
    "    model = Model(inputs=[base_model_rgb.input, base_model_depth.input], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_fcn(input_shape=(256, 256, 3), trainable_layers=[])\n",
    "model.summary()\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.008,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=1e-6)\n",
    "\n",
    "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.008, decay=1e-6, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CustomDataLoader(data_dir='../Datasets/road_scenes', batch_size=32, mode='train')\n",
    "validation_loader = CustomDataLoader(data_dir='../Datasets/road_scenes', batch_size=32, mode='validation')\n",
    "\n",
    "history = model.fit(train_loader, epochs=10, validation_data=validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We evaluate the trained model on the training and test dataset. The results are shown as: \n",
    "\n",
    "- Loss and accuracy of model for test dataset.\n",
    "\n",
    "- Prediction of semantically segmented imagesÂ on 5 random example of test dataset.\n",
    "\n",
    "- Visualization the 5 random examples alongside the ground truth and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = CustomDataLoader(data_dir='../Datasets/road_scenes', batch_size=32, mode='test')\n",
    "test_loss, test_accuracy = model.evaluate(test_loader)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5  \n",
    "\n",
    "fig, axes = plt.subplots(num_examples, 4, figsize=(12, 12))\n",
    "for i in range(num_examples):\n",
    "    rgb = np.array(cv2.resize(np.load(os.path.join('../Datasets/road_scenes', 'test', 'rgb', f'{i}.npy')), (256,256)))\n",
    "    \n",
    "    depth = np.array(cv2.resize(np.load(os.path.join('../Datasets/road_scenes', 'test', 'depth', f'{i}.npy')), (256,256))) \n",
    "    depth_rgb = np.stack((depth,) * 3, axis=-1)\n",
    "\n",
    "    prediction = model.predict([np.expand_dims(rgb, axis=0), np.expand_dims(depth_rgb, axis=0)])\n",
    "\n",
    "    predicted_mask = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    axes[i, 0].imshow(rgb)\n",
    "    axes[i, 0].set_title('RGB')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(depth)\n",
    "    axes[i, 1].set_title('Depth')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    if os.path.exists(os.path.join('../Datasets/road_scenes', 'test', 'label', f'{i}.npy')):\n",
    "        label = np.array(cv2.resize(np.load(os.path.join('../Datasets/road_scenes', 'test', 'label', f'{i}.npy')), (256,256)))\n",
    "        axes[i, 2].imshow(label)\n",
    "        axes[i, 2].set_title('Ground Truth Label')\n",
    "    else:\n",
    "        axes[i, 2].set_title('Ground Truth Label (Not Available)')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "    axes[i, 3].imshow(predicted_mask[0], cmap='jet', vmin=0, vmax=18)\n",
    "    axes[i, 3].set_title('Predicted Mask')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra fun experiment :\n",
    "\n",
    "Implement FCNs for each sing modality and compare their accuracy with fusion model. Compare the performance of the fused model to the single modality based models.\n",
    "\n",
    "Enjoy!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
